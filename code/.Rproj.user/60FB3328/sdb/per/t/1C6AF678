{
    "collab_server" : "",
    "contents" : "require(pracma)\nrequire(nloptr)\n\n#' Calculates, based on a structure of lambda history ( pairs (Time, Count) ) \n#' the average value of the number of viewcounts (lambda) at the given moment t.\n#' The lambda_history can either by a past history of event counts (observed or \n#' simulated) or NULL (default) if NULL, an artificial series is generated with \n#' parameters until time t-1, and then value at time t is generated. The \n#' external influence should be a vector of the length higher or equal than the\n#' desired period + 1 (for moment 0 of time). NULL is no influence (a value of\n#' zero).\n#' \n#' @param params (gamma, eta, K, beta, c, theta, mu1, mu2, ...) and alpha: simulation \n#'   parameters\n#' @param ext_infl: the series containing S(t) - the external influence in our \n#'   model\npredict_theoretical_lambda <- function(params = c(gamma = 100, eta = 100, K = 0.024, beta = 0.5, c = 0.001, theta = 0.2, mu1 = 1), t, \n                                       lambda_history = NULL, ext_infl = NULL, alpha = 2.016) {\n  res <- .check_fix_mus_ext_infl(params = params, ext_infl = ext_infl, t = t)\n  params <- res$params\n  ext_infl <- res$ext_infl\n  \n  if (t == 0) {\n    # remember that the first element is influence at time 0, \n    # ext_infl[i] is the influence at time i-1\n    infl_val <- 0\n    for (i in 1:length(ext_infl)) {\n      muname <- sprintf(\"mu%d\", i)\n      infl_val <- infl_val + params[[muname]] * ext_infl[[i]][1]\n    }\n    return(params$gamma + infl_val) \n  }\n  \n  if (! is.null(lambda_history)) {\n    # the event has 2 components: (magnitude_i, time_i)\n    mat_event = matrix(unlist(lambda_history), ncol = 2, byrow = F)\n    timei = mat_event[,1]\n    lambdai = mat_event[,2]\n    \n    # eliminate values which do not interest us (scope of time is above our t)\n    lambdai <- lambdai[timei < t]\n    timei <- timei[timei < t]\n    \n    # calculate the time difference between each lambdai and the current moment\n    diffi <- t - timei\n    \n    # calculate the rest of the formula\n    # 1. time decay part\n    result <- (diffi + params$c) ^ (-1-params$theta)\n    # 2. multiply the values of lambda\n    result <- result * lambdai\n    # 3. add the external influence\n    infl_val <- 0\n    for (i in 1:length(ext_infl)) {\n      muname <- sprintf(\"mu%d\", i)\n      infl_val <- infl_val + params[[muname]] * ext_infl[[i]][t+1]\n    }\n    # 4. sum everything up and finish calculations\n    result <- params$eta  + infl_val + params$K * ((alpha - 1) / (alpha - params$beta - 1)) * sum(result)\n  } else {\n    series <- generate_simulated_data(params = params, time = t, ext_infl = ext_infl)\n    result <- series$Count[series$Time == t]\n  }\n  \n  (result)\n}\n\n############################ Functions for testing the fitting on simulated data\n#' Function used to generate viewcount data (with or without noise), starting \n#' from an initial value. it calls predic_theoretical_lambda to get next value \n#' based on past values. The length of the generated list is time\n#' \n#' @param params (gamma, eta, mu, K, beta, c, theta) and alpha: simulation \n#'   parameters\n#' @param time: how many timeslices should be simulated\n#' @param ext_infl: the series containing S(t) - the external influence in our \n#'   model\n#' @param noise: should uniform noise be added to the generated series\n#' @param multiplicative: if noise, should the noise be a proportion of the \n#'   generated values, or the generated noise should simply be added to the \n#'   values E.g.: multiplicative = true: counts = counts + counts*factor, else:\n#'   counts = counts + factor\n#' @param factor: the factor of the noise\ngenerate_simulated_data <- function(params = c(gamma = 100, eta = 100, K = 0.024, beta = 0.5, c = 0.001, theta = 0.2, mu1 = 1), time,\n                                    ext_infl = NULL, alpha = 2.016, noise = FALSE, factor = 0.3, multiplicative = TRUE, prefix = NULL) {\n  # add the initial event\n  event_list <- data.frame(matrix(,nrow = time+1, ncol = 2), row.names = NULL)\n  colnames(event_list) <- c(\"Time\", \"Count\")\n  event_list$Time <- 0:time\n  if (is.null(prefix))\n    prefix <- predict_theoretical_lambda(params = params, t = 0, ext_infl = ext_infl, alpha = alpha)\n  \n  # initialize event count list\n  upper <- length(prefix)\n  if (upper > time + 1) upper <- time + 1\n  event_list$Count[1:upper] <- prefix[1:upper]\n  next_t <- upper\n  \n  # go through each moment of time\n  if ( next_t <= time) {\n    for (t in next_t:time) {\n      event_list$Count[t+1] <- predict_theoretical_lambda(params = params, t = t, lambda_history = event_list, ext_infl = ext_infl, alpha = alpha)\n    }\n  }\n  \n  # add noise to the generated series\n  if (noise) {\n    if (multiplicative) {\n      # generate gaussian proportions between -factor to factor\n      factor = abs(factor)\n      noise <- runif(n = time, min = -1 * factor, max = factor) * event_list$Count[-1]\n    } else {\n      factor = abs(factor)\n      noise <- runif(n = time, min = -1 * factor, max = factor)\n    }\n    \n    # add the noise to the generated event list\n    event_list$Count[-1] <- event_list$Count[-1] + noise\n  }\n  \n  (event_list)\n}\n\n#' this function will need to be minimized in order to fit the theoretical \n#' parameters to the real observed values\n#' \n#' @param params a list or vector with the models parameters.\n#' @param real_lambda_history the observed event counts to which to compare when\n#'   calculating error.\n#' @param ext_infl: the series containing S(t) - the external influence in our \n#'   model\n#' @param alpha (default 2.016) the value of alpha\n#' @param return_error_vector (default F) if True, return the error vector \n#'   instead of just the sum\n#' @param lowerBound (default NULL) - a vector of values which are the lower \n#'   bounds for the parameters. If a given parameter is lower than its lower \n#'   bound, than the error function is set to a very large value.\n#' @param upperBound (default NULL) - same as lowerBound, just for the upper\n#'   bound.\n#' @param alpha_regularizer (default 0) - metaparameter for regularizing the\n#'   values of parameters. If zero, no regularization.\n#' @param param_scaling (default NULL) - vector of scaling values for each\n#'   parameter. If NULL, a dummy vector of ones will be used.\nerror_function <- function(params, real_lambda_history, ext_infl = NULL, alpha = 2.016, \n                           return_error_vector = FALSE, disable_gradient_params = NULL,\n                           lowerBound = NULL, upperBound = NULL,\n                           alpha_regularizer = 0, param_scaling = NULL) {\n  \n  # obtain the parameters from the list of of parameters. This will be coming \n  # from the optimization procedure process parameters\n  ## also correct the parameters to be a names list.\n  params <- .correct_names(params)\n  \n  # check if we got bounds\n  if (is.null(lowerBound)) lowerBound <- rep(-Inf, times = length(params))\n  if (is.null(upperBound)) upperBound <- rep(Inf, times = length(params))\n  \n  # check if we got params scaling\n  if (is.null(param_scaling))\n    param_scaling <- rep(x = 1, times = length(params))\n  param_scaling <- unlist(param_scaling)\n  \n  # sanity check - check if the normalization params are the same length as the params\n  # at this stage, they should be compatible\n  if (length(params) != length(param_scaling))\n    stop(sprintf(\"[error_function]: You have %d parameters and %s normalization parameters. Cannot work in these conditions!\", length(params), length(param_scaling)))\n  \n  # calculate the theoretical values of lambda, based on our parameters and the\n  # known external influence. this boils down to simulating the series with the\n  # parameters\n  theoretical_lambda <- generate_simulated_data(params = params, \n                                                time = max(real_lambda_history$Time), \n                                                ext_infl = ext_infl, \n                                                alpha = alpha)\n  \n  # calculate the error\n  result <- (real_lambda_history$Count - theoretical_lambda$Count) ^2 / 2\n  if (return_error_vector) {\n    return(result)  \n  }\n  result <- sum(result)\n  \n  # calculate the regularization\n  #   disable_reg <- c(\"theta\", \"c\")\n  disable_reg <- c(5, 6)\n  regularized_sum <- (as.numeric(params) / param_scaling) ^ 2\n  regularized_sum[disable_reg] <- 0\n  reg <- 0.5 * alpha_regularizer * sum(regularized_sum, na.rm = T)\n  result <- result + reg\n  \n  # L-BFGS-B doesn't like Inf values for this function, therefore we will replace Inf with a ridiculously large value\n  maxValue <- .Machine$double.xmax - 1\n  \n  if ( !is.finite(result) ) result <- maxValue\n  if ( is.nan(result) ) result <- maxValue\n  if ( result > maxValue ) result <- maxValue\n  \n  # check bounds\n  for (i in 1:length(params)) {\n    if ( params[[i]] < lowerBound[i]) result <- maxValue\n    if ( params[[i]] > upperBound[i]) result <- maxValue\n  }\n  \n  return(result)\n}\n\n###################### The next section is for calculating the gradient in a closed form ####################################\n# the closed form derivatives are (in latex format):\n# \\fp{J}{K} = \\sum_{t=1}^T e(t) \\fp{\\lambda(t)}{K} ; \n#     \\fp{\\lambda(t)}{K} = \\frac{1 - \\alpha}{\\alpha - \\beta - 1} \\sum_{\\tau=1}^{t} \\hat\\lambda(t-\\tau) (\\tau + c)^{-(1+\\theta)}\n# \\fp{J}{\\beta} = \\sum_{t=1}^T e(t) \\fp{\\lambda(t)}{\\beta} ;\n#     \\fp{\\lambda(t)}{\\beta} = \\frac{1 - \\alpha}{\\left( \\alpha - \\beta - 1 \\right)^2} \\sum_{\\tau=1}^{t} \\hat\\lambda(t-\\tau) (\\tau + c)^{-(1+\\theta)}\n# \\fp{J}{c} = \\frac{1 - \\alpha}{\\alpha - \\beta - 1} (1+\\theta) \\sum_{t=1}^T e(t) \\fp{\\lambda(t)}{c} ;\n#     \\fp{\\lambda(t)}{c} = \\sum_{\\tau=1}^{t} \\hat\\lambda(t-\\tau) (\\tau + c)^{-(2+\\theta)}\n# \\fp{J}{\\theta} = \\frac{1 - \\alpha}{\\alpha - \\beta - 1} \\sum_{t=1}^T e(t) \\sum_{\\tau=1}^{t} \\hat\\lambda(t-\\tau) \\ln (\\tau+c) (\\tau + c)^{-(1+\\theta)}\n##\n##\n#' Calculates the gradient of the \\lambda(t), dependent on the values of the \n#' \\lambda(t) and of the gradient of \\lambda(0:t-1) at previous times. This \n#' gradient is necessary in the calculation of the gradient of the error \n#' function for fitting. The formula of calculation is given in version 3 of the\n#' work documents.\n#' \n#' @param params a list or vector with the models parameters.\n#' @param alpha parameter of the model, non variable for now.\n#' @param t the time at which to calculate the gradient.\n#' @param lambda_values the series of values for lambda (needed in the\n#'   calculation). If NULL, then the series is generated using the parameters.\n#' @param previous_grad_lambda the matrix containing the gradient values at\n#'   previous moments of time. If NULL, then this function will reconstruct the\n#'   matrix by recursivelly calling itself for previous moments of time.\n#' @param ext_infl: the series containing S(t) - the external influence in our \n#'   model\n#' @return the matrix, with t+1 rows and nvar columns, giving the values of the\n#'   gradient for each variable, at each moment of time <= the indicated time.\ngrad_lambda <- function(params = c(gamma = 100, eta = 100, K = 0.024, beta = 0.5, c = 0.001, theta = 0.2, mu1 = 1), t, \n                        lambda_values = NULL, previous_grad_lambda = NULL, ext_infl = NULL, alpha = 2.016) {\n  \n  # process parameters and external influence\n  res <- .check_fix_mus_ext_infl(params = params, ext_infl = ext_infl, t = t)\n  params <- res$params\n  ext_infl <- res$ext_infl\n  \n  # there is a pathological case, in which no big calculations are necesary: t = 0\n  # simply test and return what we were asked\n  if (t == 0) {\n    calculation <- data.frame(t = 0, gamma = 1, eta = 0, K = 0, beta = 0, c = 0, theta = 0, q = 0, row.names = NULL )\n    for (i in 1:length(ext_infl)) {\n      muname <- sprintf(\"mu%d\", i)\n      calculation[[muname]] <- ext_infl[[i]][1]\n    }\n    return(calculation)\n  }\n  \n  # check if we were given the lambda_values\n  if ( is.null(lambda_values) ) {\n    lambda_values <- generate_simulated_data(params = params, time = t, ext_infl = ext_infl, alpha = alpha)\n  }\n  \n  # check if we have the previous values of the gradient of lambda\n  if ( is.null(previous_grad_lambda)) {\n    # call ourselves with one less time step, to get the previous values\n    previous_grad_lambda <- NULL\n    for (time in 0:(t-1)) {\n      previous_grad_lambda <- grad_lambda(params = params, t = time, lambda_values = lambda_values, \n                                          previous_grad_lambda = previous_grad_lambda, ext_infl = ext_infl, alpha = alpha)\n    }\n  }\n  \n  # keep only what we want\n  prev_grad <- previous_grad_lambda[previous_grad_lambda$t < t,]\n  \n  # eliminate values which do not interest us (scope of time is above our t)\n  lambdai <- lambda_values$Count[lambda_values$Time < t]\n  timei <- lambda_values$Time[lambda_values$Time < t]\n  \n  # calculate the time difference between each lambdai and the current moment\n  taui <- t - timei\n  \n  # calculate the rest of the formula\n  # 1. time decay part\n  decay <- (taui + params$c) ^ (-1-params$theta)\n  decay_c <- (taui + params$c) ^ (-2-params$theta) # the decay has a slighter different form for parameter c\n  \n  # 2. get the previous grad lambda values and sum them up\n  prev_grad <- prev_grad * decay\n  prev_grad <- colSums(prev_grad, na.rm = T)[-1] #first column is Time\n  \n  # 3. multiply the values of real lambda\n  result <-  decay * lambdai #valid for q\n  result_c <- -(1 + params$theta) * sum(decay_c * lambdai) # the form for parameter c\n  # for parameter theta, we have an additional term ln(taui+c)\n  result_theta <- -1 * sum(result * log(taui+params$c))\n  result_q <- sum(result)\n  \n  # calculate final values\n  val_q = params$K * ((alpha - 1) / (alpha - params$beta - 1))\n  grad_theta <- val_q * (result_theta + prev_grad[\"theta\"])\n  grad_c <-  val_q * (result_c + prev_grad[\"c\"])\n  grad_eta = 1 + val_q * prev_grad[\"eta\"]\n  grad_q = result_q + val_q * prev_grad[\"q\"]\n  grad_K = ((alpha - 1) / (alpha - params$beta - 1)) * grad_q\n  grad_beta = (params$K * (alpha - 1)) / ((alpha - params$beta - 1)^2) * grad_q\n  grad_gamma = val_q * prev_grad[\"gamma\"]\n  \n  calculation <- data.frame(t = t, gamma = grad_gamma, eta = grad_eta, K = grad_K, \n                            beta = grad_beta, c = grad_c, theta = grad_theta, q = grad_q, \n                            row.names = NULL )\n  # calculate and add the mus\n  for (i in 1:length(ext_infl)) {\n    muname <- sprintf(\"mu%d\", i)\n    calculation[[muname]] <- ext_infl[[i]][t+1] + val_q * prev_grad[muname]\n  }\n  \n  prev_grad <- previous_grad_lambda[previous_grad_lambda$t < t,]\n  calculation <- rbind(prev_grad, calculation)\n  \n  return(calculation)\n}\n\n# using the results of the previous function, this next function will calculate\n# the closed form gradient of the error function, over the parameters\nerror_function_gradient <- function(params = c(gamma = 100, eta = 100, K = 0.024, beta = 0.5, c = 0.001, theta = 0.2, mu1 = 1), real_lambda_history, \n                                    ext_infl = NULL, alpha = 2.016, return_error_vector = FALSE, disable_gradient_params = \"beta\",\n                                    lowerBound = NULL, upperBound = NULL,\n                                    alpha_regularizer = 0, param_scaling = NULL) {\n  # obtain the parameters from the list of parameters. This will be coming from\n  # the optimization procedure process parameters\n  params <- .correct_names(params)\n  \n  # check if we got bounds\n  if (is.null(lowerBound)) lowerBound <- rep(-Inf, times = length(params))\n  if (is.null(upperBound)) upperBound <- rep(Inf, times = length(params))\n  \n  # check if we got params scaling\n  if (is.null(param_scaling))\n    param_scaling <- rep(x = 1, times = length(params))\n  param_scaling <- unlist(param_scaling)\n  \n  # what is the maximum extent of time?\n  max_time <- max(real_lambda_history$Time)\n  \n  # generate the articial series corresponding to the parameters\n  lambda_values <- generate_simulated_data(params = params, time = max_time, ext_infl = ext_infl, alpha = alpha)\n  \n  # calculate the grad of the lambda, for the given parameters, using the function defined above.\n  lambda_grad <- grad_lambda(params = params, t = max_time, lambda_values = lambda_values, ext_infl = ext_infl, alpha = alpha)\n  \n  # calculating e(t)\n  error_t <-  lambda_values$Count - real_lambda_history$Count\n  \n  return_val <- colSums(error_t * lambda_grad)\n  # we don't want the t and q parameters \n  return_val <- return_val[!(names(return_val) %in% c(\"t\", \"q\"))]\n  \n  # calculate the regularization\n  #   disable_reg <- c(\"theta\", \"c\")\n  disable_reg <- c(5, 6)\n  #   reg <- alpha_regularizer * (as.numeric(params) / param_scaling) ## wrong, but fitting results are with this\n  reg <- alpha_regularizer / param_scaling ## correct\n  reg[disable_reg] <- 0\n  return_val <- return_val + reg\n  \n  # L-BFGS-B doesn't like Inf values for this function, therefore we will replace Inf with a ridiculously large value\n  maxValue <- .Machine$double.xmax - 1\n  \n  # check bounds\n  for (i in 1:length(params)) {\n    if ( params[[i]] < lowerBound[i]) return_val[i] <- -1 * maxValue\n    if ( params[[i]] > upperBound[i]) return_val[i] <- maxValue\n  }\n  \n  return_val[is.na(return_val)] <- maxValue\n  return_val[is.nan(return_val)] <- maxValue\n  return_val[return_val > maxValue] <- maxValue\n  return_val[!is.finite(return_val)] <- maxValue\n  \n  if (!is.null(disable_gradient_params)) {\n    #   return_val[\"eta\"] <- 0 # this is how you exclude a variable from fitting\n    for ( i in disable_gradient_params) return_val[i] <- 0\n  }\n  \n  return(return_val)\n}\n\n# calculate the gradient of the function, using the finite differences. Do the\n# calculation in parallel\nparallel_gradient <- function(.params, ...) { # Now use the cluster \n  dp = cbind(rep(0,length(.params)),diag(.params * 1e-8));   \n  #   Fout = parCapply(.cl, dp, function(x) fn(.params + x, ...)); # Parallel \n  Fout = apply(dp, MARGIN = 2, function(x) error_function(.params + x, ...)); # Parallel \n  return((Fout[-1]-Fout[1])/diag(dp[,-1]));                  #\n}\n\n# artificial data simulation\nfit_artificial_data <- function(params = c(gamma = 100, eta = 100, K = 0.024, beta = 0.5, c = 0.001, theta = 0.2, mu1 = 1), ext_infl = NULL,\n                                alpha = 2.016, time = 100, maxit = 100, noise = FALSE, factor = 0.3, multiplicative = TRUE) {\n  # process parameters\n  params <- .check_fix_mus_ext_infl(params = params, ext_infl = ext_infl)$params\n  \n  # first set the parameters and compute various measures\n  n <- get_n(K = params$K, beta = params$beta, c = params$c, theta = params$theta) # no longer defined\n  q <- params$K * ((alpha - 1) / (alpha - params$beta - 1))\n  pars <- data.frame(params, q = q, n = n, row.names = \"initial\")\n  #   pars <- data.frame(gamma = params$gamma, eta = params$eta, K = params$K, beta = params$beta,  \n  #                      q = q, c = params$c, theta = params$theta, n = n)\n  #   rownames(pars) <- \"initial\"\n  # generate the simulate series and fit the curve using our fitting method\n  gen_series <- generate_simulated_data(params = params, \n                                        time = time, \n                                        ext_infl = ext_infl,\n                                        alpha = alpha,\n                                        noise = noise, \n                                        factor = factor, \n                                        multiplicative = multiplicative)$Count\n  \n  results_gen_series <- fit_series(data_series = gen_series, \n                                   initial_params = c(0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1), \n                                   ext_infl = ext_infl, alpha = alpha, method = \"nloptr\" )$model\n  #   #   print(gen_series)\n  #   results_gen_series <- optim( par = unlist(.correct_names(c(0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1))), ## easy to fit starting from perfect parameters, no?: unlist(params), \n  #                                fn = error_function,\n  #                                gr = error_function_gradient,\n  #                                real_lambda_history = gen_series, \n  #                                ext_infl = ext_infl,\n  #                                control = list(maxit = maxit),\n  #                                method = \"BFGS\")\n  #   # check convergence\n  #   if (results_gen_series$convergence != 0) {\n  #     warning(sprintf(\"The fitting did not converge. The message is: '%s'. Try increasing the number of iterations using the maxit parameter!\", results_gen_series$message))\n  #   }\n  \n  # compute the measures using the fitted values and put them into the same data frame\n  q_fit <- results_gen_series$par[\"K\"] * ((alpha - 1) / (alpha - results_gen_series$par[\"beta\"] - 1))\n  n_fit <- get_n(K = results_gen_series$par[\"K\"], beta = results_gen_series$par[\"beta\"], c = results_gen_series$par[\"c\"], theta = results_gen_series$par[\"theta\"])\n  pars <- rbind(pars, data.frame( as.list(results_gen_series$par), q = q_fit, n = n_fit, row.names = \"fitted\"))\n  \n  # print the results\n  print(pars, digits=4)\n  \n  return(results_gen_series)\n}\n\n# this function calculates the value of q, based on K and beta.\nget_q <- function(K = 0.024, beta = 0.5, alpha = 2.016) {\n  result <- K * ((alpha - 1) / (alpha - beta - 1))\n  \n  return(result)\n}\n\n#' The next functions are to fit arbitrary series of real data. Internally, it \n#' constructs the real data series + timestamp, and calls the optimization \n#' procedure (with the method in the parameter).\n#' \n#' @param data_series - an array of numbers which are to be fitted\n#' @param init_params ( default c(gamma = 100, eta = 100, K = 0.0024, beta = 1, \n#'   c = -0.73, theta = 3.35, mu1 = 1) )- the initial parameters in the search \n#'   space\n#' @param lowerBound (default NULL) - a vector of values which are the lower \n#'   bounds for the parameters. If a given parameter is lower than its lower \n#'   bound, than the error function is set to a very large value (for optim, \n#'   nloptr does it itself)\n#' @param upperBound (default NULL) - same as lowerBound, just for the upper \n#'   bound.\n#' @param alpha_regularizer (default 0) - metaparameter for regularizing the \n#'   values of parameters. If zero, no regularization.\n#' @param param_scaling (default NULL) - vector of scaling values for each \n#'   parameter. If NULL, a dummy vector of ones will be used.\n#' @params disable_optim_params (default NULL) - parameters given in this array \n#'   will not be optimized and will remain at the initial values\n#' @params method (options: c(\"nloptr\", \"optim\") - default \"nloptr\") the method\n#'   used to implement the gradient descent. \"nloptr\" is based on the \"nloptr\"\n#'   library, it is faster and has included bound checks.\n#'   \n#' @return a vector of number (the generated counts from the fitted parameters) \n#'   and the fitted model as returned by the optimization procedure.\nfit_series <- function(data_series, initial_params = c(gamma = 100, eta = 100, K = 0.0024, beta = 1, c = 0.01, theta = 3.35, mu1 = 1), \n                       ext_infl = NULL, alpha = 2.016, disable_relaxation_kernel = F, lowerBound = NULL, upperBound = NULL,\n                       alpha_regularizer = 0, param_scaling = NULL, disable_optim_params = \"beta\", method = \"nloptr\") {\n  # process parameters\n  initial_params <- .check_fix_mus_ext_infl(params = initial_params, ext_infl = ext_infl)$params\n  \n  # first compose the data structure that we need for fitting\n  counts <- data.frame(Time = 1:length(data_series)-1, Counts = data_series)\n  \n  # if we are to disable the relaxation kernel, we set K to zero and make sure it is not updated in the gradient\n  disable_gradient_params = disable_optim_params\n  if (disable_relaxation_kernel) {\n    initial_params[\"K\"] <- 0\n    disable_gradient_params <- c(disable_gradient_params, \"K\", \"beta\", \"theta\", \"c\")\n  }\n  \n  ## check method\n  if (! method %in% c(\"nloptr\", \"optim\")) {\n    warning(sprintf(\"Method %s not recognised. Defaulting to \\\"nloptr\\\"\", method))\n    method <- \"nloptr\"\n  }\n  \n  ##################### DONE checking. Start fitting.\n  if (method == \"nloptr\") {\n    ## alternativelly, use the library \"nloptr\" lbfgs implementation (implements\n    ## L-BFGS). Note that the lbfgs function in \"nloptr\" does the bound checking on\n    ## its own, so no need to pass it to the error function hack\n    fitted_model <- NULL\n    require(nloptr)\n    tryCatch(\n      fitted_model <- lbfgs(x0 = unlist(initial_params),\n                            fn = error_function, \n                            gr = error_function_gradient, \n                            lower = lowerBound, upper = upperBound,\n                            control = list(xtol_rel = \"1e-8\", maxeval = 1000, check_derivatives = F), ## TODO: maybe lower maxeval for faster (less accurate fitting)\n                            real_lambda_history = counts,  ## from here, our function parameters\n                            alpha = alpha,\n                            ext_infl = ext_infl,\n                            disable_gradient_params = disable_gradient_params,\n                            alpha_regularizer = alpha_regularizer, param_scaling = param_scaling) ,\n      error = function(e) warning(paste(\"There was an error: \", e)))\n    \n    # check convergence\n    if (!is.null(fitted_model) & fitted_model$convergence < 0) {\n      warning(sprintf(\"The fitting did not converge. The message is: '%s'. Try increasing the number of iterations using the maxit parameter!\", fitted_model$message))\n      fitted_model <- NULL ## fallback to optim\n    }\n    \n    ## if something went wrong, fallback to optim\n    if (is.null(fitted_model)) {\n      warning(\"\\\"nloptr\\\" did not succeed! Trying fallback optim!\")\n      method <- \"optim\"\n    }\n  }\n  \n  if (method == \"optim\") {\n    ## next, fit the parameters of our model - using R supplied BFGS\n    fitted_model <- NULL\n    tryCatch(\n      fitted_model <- optim( par = initial_params,\n                             fn = error_function,\n                             gr = error_function_gradient,\n                             real_lambda_history = counts, \n                             alpha = alpha,\n                             ext_infl = ext_infl,\n                             disable_gradient_params = disable_gradient_params,\n                             lowerBound = lowerBound, upperBound = upperBound,\n                             alpha_regularizer = alpha_regularizer, param_scaling = param_scaling,\n                             control = list(maxit = 5000),\n                             method = \"BFGS\") ,\n      error = function(e) warning(paste(\"There was an error: \", e)))\n    \n    # check convergence\n    if (!is.null(fitted_model) & fitted_model$convergence != 0) {\n      warning(sprintf(\"The fitting did not converge. The message is: '%s'. Try increasing the number of iterations using the maxit parameter!\", fitted_model$message))\n    }\n  }\n  \n  if (is.null(fitted_model)) return(list(fitted_counts = rep(NA, times = nrow(counts), model = NA)))\n  \n  #   print(fitted_model)\n  \n  ## if here, all went alright.\n  ## in case of nloptr lbfgs, we need to reinstate the model's parameter names.\n  names(fitted_model$par) <- names(initial_params)\n  \n  # generate a artificial sequence with the fitted parameters\n  fitted_counts <- generate_simulated_data(params = fitted_model$par, time = nrow(counts)-1, ext_infl = ext_infl, alpha = alpha)$Count\n  \n  # and return the fitted counts and model\n  result <- list(fitted_counts = fitted_counts, model = fitted_model)\n  return(result)\n}\n\n#' Internal usage method only. It checks that the parameters \"mu_i\" are in sync \n#' with the external influences received by the algorihms. The purpose is to \n#' have as many parameters as internal influence series. If t is provided \n#' (default NULL), than the external influences will be tailored to the length \n#' required, if needed by adding zeros.\n#' \n#' @param params - list or numeric vector, containing parameters. 6 default \n#'   parameters (gamma, eta, K, beta, c, theta) and a variable number of mu \n#'   parameters (mu1, mu2 ...)\n#' @param ext_infl (default NULL)- a list of series which are the external\n#'   influence\n#' @param t (default NULL) - the time length for which to tailor the external\n#'   influence\n#'   \n#' @return a list containing the new parameters and the new list of external\n#'   influences.\n.check_fix_mus_ext_infl <- function(params, ext_infl = NULL, t = NULL, lBound = 0, uBound = 505.90) {\n  # process parameters\n  params <- .correct_names(params)\n  \n  # deal with the case of no external influence (we got a NULL parameter)\n  if (is.null(ext_infl)) {\n    ext_infl <- list(rep(x = 0, times = 2))\n  }\n  \n  # by default we have only one series of external influence. Make sure that if\n  # we have more, than the number of mus is identical the parameters are:\n  # (gamma, eta, K, beta, c, theta, mu1, mu2, ...). 6 fixed ones + as many mus\n  # as necessary\n  \n  # if less parameters than external influences => add parameters\n  if (length(ext_infl) + 6 > length(params)) {\n    # we need more mus. add with warning\n    #     warning(sprintf(\"Insufficiant mu parameters. I got %d external series and only %d mu params. Adding the rest.\", length(ext_infl), length(params)-6))\n    existing_mus <- length(params) - 6\n    for (i in 1:(length(ext_infl) + 6 - length(params))) {\n      existing_mus <- existing_mus + 1\n      varname <- sprintf(\"mu%d\", existing_mus)\n      params[[varname]] <- runif(n = 1, min = lBound, max = uBound) #initially 1\n    }\n  }\n  \n  # if more parameters than external influences => add dummy external influence\n  if (length(ext_infl) + 6 < length(params)) {\n    needed_series <- length(params) - 6 - length(ext_infl)\n    for (i in 1:needed_series) {\n      ext_infl <- c(ext_infl, list(NA))\n    }\n  }\n  \n  # if t is provided, make sure that each external influence is tailored to the desired length t+1 (time starts at 0)\n  if (!is.null(t)) {\n    # the external influence should be a vector of the same length as the desired period\n    ext_infl <- lapply(X = ext_infl, \n                       FUN = function(x) {\n                         x <- unlist(x)\n                         if (length(x) > t+1) x <- x[1:(t+1)]\n                         if (length(x) < t+1) x <- c(x, rep(x = NA, times = t + 1 - length(x)))\n                         x[!is.finite(x)] = 0\n                         \n                         return(x)\n                       })\n  }\n  \n  return(list(params = params, ext_infl = ext_infl))\n}\n\n##################################### SIMULATION OF THE HELMSTETTER MODEL ##############################################\n.get_n <- function(params, alpha = 2.016, mmin = 1) {\n  return(get_n(K = params$K, beta = params$beta, c = params$c, theta = params$theta, alpha = alpha, mmin = mmin)) \n}\n\n# the get_n function calculates the average number of events generated by an event\n# useful to detect super-critical regimes\nget_n <- function(K = 0.024, alpha = 2.016, beta = 0.5, mmin = 1, c = 0.001, theta = 0.2) {\n  \n  if (!is.finite(K * alpha * beta * mmin * c * theta))\n    return(NA)\n  \n  if (beta >= (alpha - 1) ) {\n    warning(\"The closed expression calculated by this function does NOT hold for beta >= alpha - 1\")\n    \n    return (NA)\n  }\n  \n  if (theta <= 0 ) {\n    warning(sprintf(\"The closed expression calculated by this function does NOT hold for theta <= 0 (K=%.4f, beta=%.2f, theta=%.2f)\", K, beta, theta))\n    \n    return (NA)\n  }\n  \n  n0 = (K * (alpha - 1)) / (alpha - 1 - beta)\n  int = 1 / ( theta * c^theta)\n  #     if (theta > 0.001)\n  #         int1 = integrate(f=function(t) 1/((t+c)^(1+theta)),lower=0,upper=Inf)$value\n  #     else\n  #         int1 = int\n  #     \n  #     if ( abs(int - int1) >= 0.00001) {\n  #         warning(sprintf(\"Integral (val=%.4f) and closed form (val=%.4f) solution in calculating n do not give the same result! Params: theta: (%.4f\", int1, int, theta))\n  #     }\n  \n  n = n0 * int\n  \n  return (n) \n}\n\n#' Calculates the time-decaying kernel corresponding to a given event/set of \n#' events, at the givent time. The mernel function is the probability that an \n#' event is spawned at the current time, considering the previous events given \n#' as parameters.\n#' \n#' @param event - the event is a 2 elements list (mi, ti), magnitude and arrival\n#'   time of the event.\n#' @param t - time at which to calculate the the probability.\n#' @param params - list of parameters (see above). Note that gamma, eta and mui\n#'   are not necessary here.\nkernel_fct <- function(event, t, params = list( K = 0.024, beta = 0.5, c = 0.001, theta = 0.2), mmin = 1, alpha = 2.016 ) {\n  params <- .correct_names(params)\n  \n  # the event has 2 components: (magnitude_i, time_i)\n  mat_event = matrix(unlist(event), ncol = 2, byrow = F)\n  mi = mat_event[,1]\n  ti = mat_event[,2]\n  \n  # f(p_j) part - virality of a video. Constant for a given video\n  fun_f <- params$K\n  \n  # ro(m_i) part - the influence of the user of the event\n  fun_ro <- (mi / mmin) ^ params$beta\n  \n  # psi(t, ti) part - the decaying / relaxation kernel\n  fun_psi <- 1 / (t - ti + params$c)^(1 + params$theta)\n  \n  val = fun_f * fun_ro * fun_psi\n  val[t<ti] = 0\n  val[mi<mmin] = 0\n  \n  (val)\n}\n\n#' Calculate the total number of events generated by a single event, of\n#' magnitude 1 at time 0. This is done by numerical integration of the generated\n#' curve and it allows, for super-critical videos, to determine the\n#' characteristic time.\n#' \n#' @return a list with 3 components: endo - the endogenous reponse (the number\n#'   of events as a response to a single event of mag 1 at time 0), n -\n#'   branching factor and the characteristic time\nget_endogenous_response <- function(params = c(gamma = 1, eta = 0, K = 0.024, beta = 0.5, c = 0.001, theta = 0.2, mu1 = 1), \n                                    alpha = 2.016, mmin = 1, max_simulation_time = 10000) {\n  params <- .correct_names(params)\n  n <- .get_n(params = params, alpha = alpha, mmin = mmin)\n  # remember: one event of mag 1 at time 0\n  params$gamma <- 1\n  params$eta <- 0\n  series <- generate_simulated_data(params = params, time = max_simulation_time, alpha = alpha)$Count\n  \n  endo <- NULL\n  tryCatch( \n    endo <- quadl(f = function(prs) { sapply(X = prs, FUN = function(x) return(series[(floor(x)+1)])) }, \n                  xa = 0, xb =  max_simulation_time),\n    error = function(e) warning(paste(\"There was an error: \", e)))\n  if (!is.numeric(endo)) endo <- NA\n  \n  ## if integration failed, try the default method\n  if (is.na(endo)) {\n    tryCatch( \n      endo <- integrate(f = function(prs) { sapply(X = prs, FUN = function(x) return(series[(floor(x)+1)])) }, \n                        lower = 0, upper = max_simulation_time, subdivisions=max(50000, max_simulation_time) )$value,\n      error = function(e) warning(paste(\"There was an error: \", e)))\n    if (!is.numeric(endo)) endo <- NA\n  }\n  \n  ## if still bad, just use a poor aproximation, not to return NA\n  if (is.na(endo)) {\n      endo <- sum(series, na.rm = T)\n  }\n  \n  ## determine the characteristic time by substracting consequtive values from series\n  ## in other words I want the first non-negative difference\n  characteristic_time <- which(series[2:length(series)] - series[1:(length(series)-1)] > 0)[1]\n  \n  return( list(endo = endo, endo_sum = sum(series), n = n, characteristic_time = characteristic_time))\n}\n\n#' This is an internal function required to correct the names of the parameters.\n#' It is needed because the \"nloptr lbfgs\" loses the parameter names. Even when\n#' passed an initial parameter named array, it loses the names when recomputing\n#' them (for example when exploring the parameter space.) The purpose is to\n#' check if the parameter array has the required names. If not, they will be\n#' attributed the default names needed for this library. The parameter names is\n#' assumed to be in order: \n#'      c(\"gamma\", \"eta\", \"K\", \"beta\", \"c\", \"theta\", \"mu1\", \"mu2\", ...)\n.correct_names <- function(params) {\n  ## first make sure the params are a list\n  params <- as.list(unlist(params))\n  ## second make sure they have names attached\n  if (is.null(names(params))) {\n    par_names <- c(\"gamma\", \"eta\", \"K\", \"beta\", \"c\", \"theta\")\n    lng <- length(par_names)\n    for (i in (lng+1):length(params)) {\n      par_names <- c(par_names, sprintf(\"mu%d\", i-lng))\n    }\n    names(params) <- par_names\n  }\n  \n  return(params)\n}\n",
    "created" : 1490833875345.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2094856941",
    "id" : "1C6AF678",
    "lastKnownWriteTime" : 1489579302,
    "last_content_update" : 1489579302,
    "path" : "~/Work/Articole/Publicatii/2015.03 HIP in WWW/github-code-data-repo/code/functions-fitting-data.R",
    "project_path" : "functions-fitting-data.R",
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}